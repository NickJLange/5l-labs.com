---
slug: vibe-coding-insanity
title: Private Agents -Public Vibe Coding and Wrangling MCP for TOIL reduction
authors: [njl]
tags: [blog, mcp, python, gemini, claude, warp]
---


# Wrangling MCP for TOIL reduction

#### Prerequisite thoughts
 * include "not_going_to_agi_soon.md"
 * include "Private agents are good!"

## What is the problem I am solving?

When describing my cloud-free internet set up at home, a distinguished former manager congratulated me on my pro-bono secondary IT job and wondered how I'd find the time. As I was still in the honeymoon phase and having fun, I generally dismissed the comment.

A few years on, the risk of "set-it-and-forget-it" is real. Recent advances in machine learning make it easier to lower the burden of keeping code up to date at home, but they rely on public cloud LLMs to operate which has issues with:

1. Data privacy
2. Data sovereignty
3. Data security
<!-- truncate -->
<!--
 Distributed Training
 Private LLMs

-->

## How I am thinking about MCP / Coding Assistants and Private Agents

**PSA**: A continued shout to the fine folks [Latent Space](https://discord.gg/XVfBxerR) - Thursdays Paper Chats and Fridays AI In Action talks are a great way to stay up to date on the latest developments in the field for busy people.

### Can we leverage machine learning and natural language to put "no code" wrappers in front of home-based services?

My first reaction to the [deep dive into MCP](https://www.youtube.com/watch?v=kQmXtrmQ5Zg) during the AI.engineer summit in NY was that it was just API-wrapping fluff; however, I quickly realized that if done right, the real value of MCP could be using Natural Language to be the "truth" of the MCP Server/Client code, independent of the underlying API. Assuming we can solve for the crazy uncle problem of LLMs:

1. Keep just the "Generator" Language and a .patch file in the repo
1. Use a MoE Strategist/Teacher/Student pipeline to update the MCP Client/Server Hooks
1. Use actual previous calls to do regression testing
1. Update Repo / Release

At this point in time, I do not see an easy way to do this work with an open source LLM (yet!) but perhaps with enough models generated in this framework, that could change.


In our next post...

#### Can we shrink the size of the model using fine-tuning, LoRa and quantization to fit on a Raspberry Pi 4 with reasonable performance?

#### How do those flowers federated learning folks fit in?

<!--

https://drive.google.com/file/d/1xMohjQcTmQuUd_OiZ3hB1r47WB1WM3Am/view

1. Use the actual previous calls to confirm the linguistic dimensions of possible intent


```mermaid
flowchart TD
    A[Rescan API] - -> B[Regenerate MCP Server w/ versioning]
    B -\-> C[Apply .patch]
    C \- -> D[Re-Run Test Cases / Evals]
    D - -> E[Publish]

```

-->
