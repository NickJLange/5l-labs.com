---
slug: jarvis2.0
title: Thoughts on Jarvis 2.0
authors: [njl]
tags: [vllm-sr, semantic-router, privateai, llm, agent, home-automation]
description: Conceptualizing "Jarvis 2.0"—a private, multi-modal AI assistant for the home, utilizing semantic routing and local speech recognition.
embedding_url: /embeddings/applied-home-ml-iot/jarvis20.embedding.json
---

# Private multi-modal AI assistant thoughts (late 2025)

<!-- truncate -->

Keeping pace is impossible; the late 2025 landscape is dominated by multi-modal models that can finally "see" and "hear" with low enough latency for a home assistant. I've just now finally caught up on the "news" and "AI/news" feed in my inbox—at the expense of punting some updates to podcasts and open web tabs.

There are a lot of cool developments that others have been working on, and here is where I'd like to spend any free time I get over the holidays:

### Jarvis 2.0: The Path to Private Agency

The core of "Jarvis 2.0" is moving from a reactive chat interface to a proactive, context-aware agent. My focus for the holidays is to integrate **Semantic Router** for lightning-fast intent detection and **vLLM-SR** (vLLM Speech Recognition) for local, low-latency voice commands that stay entirely within my home's firewall.

**What makes it "2.0"?**
- **Proactive Intelligence**: Moving beyond basic voice commands to an agent that suggests actions based on observed context and historical patterns.
- **Local Vision**: Integrating models like Qwen2-VL to understand the visual state of the house via the GarageCam/HomeKit camera network.
- **Zero Cloud**: Ensuring every byte of audio, video, and text remains on the local M2/RPi mesh, with zero external dependencies for core functionality.

<!-- *This post was cleaned up with Automation to clarify thoughts for the reader.* -->
